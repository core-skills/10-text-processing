{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyRjwrQynR22"
   },
   "source": [
    "# Week 12.2.2  - Generating word embeddings using Gensim Word2Vec\n",
    "## Word Embeddings and Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, rather than loading word embeddings trained on large generic datasets, we train our own embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_r8zsx0JudOO"
   },
   "source": [
    "![alt text](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png)\n",
    "\n",
    "[analyticsvidhya](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfoKOGconR29"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "This notebook is based on the Kavita Ganesan's tutorial, \n",
    "\n",
    "http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/\n",
    "\n",
    "with the dataset `reviews_data.txt.gz` available at: \n",
    "\n",
    "https://github.com/kavgan/data-science-tutorials/tree/master/word2vec\n",
    "\n",
    "The dataset is a single text file concatenated from 259K full hote reviews from OpinRank. The full OpinRank dataset also has car reviews, which is available at the UCI Machine Learning Repository.\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/opinrank+review+dataset\n",
    "\n",
    "#### Hotel Reviews\n",
    "\n",
    "- Full reviews of hotels in 10 different cities (Dubai, Beijing, London, New York City, New Delhi, San Francisco, Shanghai, Montreal, Las Vegas, Chicago) \n",
    "- There are about 80-700 hotels in each city \n",
    "- Extracted fields include date, review title and the full review \n",
    "- Total number of reviews: ~259,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PliE9yXYnR25"
   },
   "outputs": [],
   "source": [
    "# Import required packages and setup logging functionality\n",
    "import bokeh\n",
    "import gzip\n",
    "import gensim \n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1585115707358,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "AfWb9kyLnR2_",
    "outputId": "f7830259-b243-4038-d4ea-ff1e9a778e4f"
   },
   "outputs": [],
   "source": [
    "# The data_file path will be different depending on where you've copied the notebooks...\n",
    "data_file='../data/reviews_data.txt.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpNZ85DusF_9"
   },
   "source": [
    "### Read hotel reviews data\n",
    "\n",
    "The original format of the hotel review data is in .zip which is composed of numerous .json data files, these are read into memory in iteratively. Furthermore, a gensim utility function `gensim.utils.simple_preprocess()` is used to pre-process the text as it's read.\n",
    "\n",
    "`simple_preprocess()` converts a document (in this case a review) into a list of tokens that are lower cased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents may take a minute or two to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50924,
     "status": "ok",
     "timestamp": 1585115760731,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "mCkIeSypnR3D",
    "outputId": "2f6e3a26-ddba-45ba-b6ac-bc502372d3d7"
   },
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f): \n",
    "            # Do some pre-processing and return a list of words for each review text \n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "\n",
    "# Read the tokenized reviews into a list each review item becomes a series of words so this becomes a list of lists\n",
    "documents = list(read_input(data_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5YUtIH_nR3J"
   },
   "source": [
    "## Understanding some of the parameters\n",
    "\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model:\n",
    "`model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)`\n",
    "\n",
    "#### Parameters of Interest\n",
    "`size`<br>\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. Typical sizes are 100-300. A value of 100-150 has worked well for me. \n",
    "\n",
    "`window`<br>\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "\n",
    "`min_count`<br>\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "`workers`<br>\n",
    "How many threads to use behind the scenes?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301603,
     "status": "ok",
     "timestamp": 1585117062350,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "lA_H9mfInR3L",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "83e9a379-d2a4-4e05-b2ce-512869fdb787"
   },
   "source": [
    "model = gensim.models.Word2Vec(documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents), epochs=10)\n",
    "model.save(\"../data/word2vec/word2vec_reviews.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the word embeddings in this notebook, we can load ones that have already been trained for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "modelpath = Path('../data/word2vec/word2vec_reviews.bin').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c32b7099fcf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(str(modelpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cC0cMRZnR3P"
   },
   "source": [
    "# Looking at outputs\n",
    "- How are common words representated as vectors?\n",
    "- Can we find similarities between words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUZUZ6JWvI1U"
   },
   "source": [
    "### Words represented as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301597,
     "status": "ok",
     "timestamp": 1585117062355,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "0m_4auJdvOea",
    "outputId": "4985a791-2cb3-4318-ad1d-653284363f92"
   },
   "outputs": [],
   "source": [
    "# Get vector representation of a word\n",
    "testWord = 'dirty'\n",
    "print(f'Word - {testWord}\\nVector Representation\\n{model.wv.get_vector(testWord)}')\n",
    "print(f'Shape of vector: {model.wv.get_vector(testWord).shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7qUYXmZv7CD"
   },
   "source": [
    "### Finding similarities between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301911,
     "status": "ok",
     "timestamp": 1585117062681,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "mT-2WDWsnR3R",
    "outputId": "f0e2140f-2666-4ead-9382-903666f12b56"
   },
   "outputs": [],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301899,
     "status": "ok",
     "timestamp": 1585117062682,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "bE4bVMGfnR3Y",
    "outputId": "4035ea5b-9d7e-43d4-f38c-fb653f986c01"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301888,
     "status": "ok",
     "timestamp": 1585117062683,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "VOq0zzvGnR3c",
    "outputId": "d3384fbb-8d85-4472-b329-fe10b2b1be70"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301878,
     "status": "ok",
     "timestamp": 1585117062684,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "LmAkHmOqnR3f",
    "outputId": "5ab1e920-b9ad-49dd-d134-9c6110f88540"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301826,
     "status": "ok",
     "timestamp": 1585117062684,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Z9-X6bgYnR3l",
    "outputId": "37054ef7-07c7-4087-aa10-a45c53fc95ca"
   },
   "outputs": [],
   "source": [
    "# Get everything related to stuff on the bed\n",
    "w1 = ['bed','sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar(positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301816,
     "status": "ok",
     "timestamp": 1585117062685,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Hy6BPFqBnR3p",
    "outputId": "a81165a7-c9c9-4015-fc8f-c5fc6c9c786a"
   },
   "outputs": [],
   "source": [
    "# Similarity between two different words\n",
    "print(f'Similarity between the words dirty and smelly: {model.wv.similarity(w1=\"dirty\",w2=\"smelly\"):0.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301796,
     "status": "ok",
     "timestamp": 1585117062685,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "rGb75GUznR3t",
    "outputId": "03339e12-a10b-4f58-f66c-1e3e07ca4f26"
   },
   "outputs": [],
   "source": [
    "# Similarity between two identical words\n",
    "print(f'Similarity between the same word dirty: {model.wv.similarity(w1=\"dirty\",w2=\"dirty\"):0.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301780,
     "status": "ok",
     "timestamp": 1585117062686,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "CavnmhyKnR3w",
    "outputId": "c635b945-00bc-4bee-8843-1a83ee64593e"
   },
   "outputs": [],
   "source": [
    "# similarity between two unrelated words\n",
    "print(f'Similarity between the words dirty and clean: {model.wv.similarity(w1=\"dirty\",w2=\"clean\"):0.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301769,
     "status": "ok",
     "timestamp": 1585117062686,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "3PAyBhoEnR3z",
    "outputId": "6da594e8-ee85-4a08-ae28-772317c1f044"
   },
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "oddOneOutList = [\"cat\", \"dog\", \"france\"]\n",
    "print(f'Which words doesnt belong in the set [cat, dog, france]? Odd one out: {model.wv.doesnt_match(oddOneOutList)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301758,
     "status": "ok",
     "timestamp": 1585117062686,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "veE4VDR_nR33",
    "outputId": "9ba6787f-0c32-4f72-f18a-1fef0789c9a2"
   },
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "oddOneOutList = [\"bed\",\"pillow\",\"duvet\",\"shower\"]\n",
    "print(f'Which words doesnt belong in the set [bed, pillow, duvet, shower]? Odd one out: {model.wv.doesnt_match(oddOneOutList)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJ0FBQUrnR38"
   },
   "source": [
    "## Visualising the word vectors in 2D space\n",
    "\n",
    "Here we use a dimensionality reduction and visualisation package from Scikit-Learn, t-Distributed Stochastic Neighbor Embedding (t-SNE), which is particularly suited for visualising high dimensional data. Another popular options for dimensionality reduction is Principal Component Analysis (PCA).\n",
    "\n",
    "Note: the dimension of our word vectors by choice is 150, typical numbers could be 50, 100, and 300. Among these numbers, a dimension of 300 has been shown as most effective in capturing the syntatic and semantic information of a word. However, it will take much longer to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3yKAcTTnR39"
   },
   "outputs": [],
   "source": [
    "# Load required statistical package\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# Need the interactive Tools for Matplotlib\n",
    "%matplotlib inline\n",
    "# Plot formatting\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "font = {'size':16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NQbN-AynR4B"
   },
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,150), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.title(f'Words closest to: {word}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIHec5Phx4Ll"
   },
   "source": [
    "### Visualising closest words in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1302471,
     "status": "ok",
     "timestamp": 1585117064024,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "YyKBFzzVnR4E",
    "outputId": "77fc97fe-e27b-4509-99f8-849cbd6af4b3"
   },
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model.wv, \"bed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9nDZAdKy_B1"
   },
   "source": [
    "# Generating word embeddings for domain-specific inforamtion\n",
    "Word embeddings are interesting and thought provoking but\n",
    "- What insights can we gain from them?\n",
    "- How do we apply them to industry data?\n",
    "\n",
    "This section of the notebook will explore word embeddings and their application to geological survey data (GSWA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5MWs23uzPUP"
   },
   "source": [
    "## Loading a small sample of Geological Survey of Western Australia (GSWA) data\n",
    "Note: we have already added our Google Drive to the notebook, so we don't need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9yX-kZ10crm"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cn-cuSk40cjo"
   },
   "outputs": [],
   "source": [
    "data_file=\"../data/wamex_xml.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2721,
     "status": "ok",
     "timestamp": 1585118724590,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "S2zNT1qWzPMt",
    "outputId": "26a9d507-1663-4e9e-d59f-7cecd158a83c"
   },
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "data = list()\n",
    "with zipfile.ZipFile(data_file, \"r\") as z:\n",
    "    #df = [pd.read_json(filename) for filename in z.namelist()]\n",
    "    print(len(z.namelist()))\n",
    "    for filename in z.namelist():\n",
    "        # print(filename)\n",
    "        # df = pd.read_json(filename)\n",
    "        with z.open(filename) as f:\n",
    "            # load the json file\n",
    "            # The resulting `content` is a list\n",
    "            content = json.loads(f.read()) \n",
    "            # Convert content to a string   \n",
    "            content = \"\".join(content)\n",
    "            # Add to the data list\n",
    "            data.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2399,
     "status": "ok",
     "timestamp": 1585118724592,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Cph4A7YQzPKA",
    "outputId": "22330ef8-001b-4a61-b4a8-7df00695e3e8"
   },
   "outputs": [],
   "source": [
    "# Previewing the data that we have loaded. This is very different to the hotel reviews dataset.\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RE0Yjy_gzPHe"
   },
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in zip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "\n",
    "    data = list()\n",
    "    with zipfile.ZipFile(input_file, \"r\") as z:\n",
    "    #df = [pd.read_json(filename) for filename in z.namelist()]\n",
    "        print(len(z.namelist()))\n",
    "        for i, filename in enumerate(z.namelist()):\n",
    "            # print(filename)\n",
    "            # df = pd.read_json(filename)\n",
    "            if (i%100==0):\n",
    "                logging.info (\"read {0} reports\".format (i))\n",
    "            with z.open(filename) as f:\n",
    "                # load the json file\n",
    "                # The resulting `content` is a list\n",
    "                content = json.loads(f.read()) \n",
    "                # Convert content to a string   \n",
    "                content = \"\".join(content)\n",
    "                if len(content) >= 10:\n",
    "                    # Add to the data list\n",
    "                    yield gensim.utils.simple_preprocess (content)\n",
    "                else:\n",
    "                    logging.info(\"removed {0} because of small size\".format (filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9455,
     "status": "ok",
     "timestamp": 1585118732100,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "IIgxeD4XzPEn",
    "outputId": "f3380ec9-c5f4-456c-d7f0-c8d4b3a8bec2"
   },
   "outputs": [],
   "source": [
    "# Read the tokenized reviews into a list each review item becomes a series of words so this becomes a list of lists\n",
    "documents = list(read_input (data_file))\n",
    "logging.info(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9324,
     "status": "ok",
     "timestamp": 1585118732101,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "ok-NJAsoyBEr",
    "outputId": "1a98cad1-cc07-439b-8303-57750559aa1d"
   },
   "outputs": [],
   "source": [
    "# Review the first documents top 25 words. See how they have been pre-processed and tokenized.\n",
    "print(documents[0][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOHp-kJ01Tt2"
   },
   "source": [
    "### Training word embedding model off of domain-specific text.\n",
    "\n",
    "Note: we are required to use bigrams to aid our model due to domain-specific terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-qVxg6foJMN"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183770,
     "status": "ok",
     "timestamp": 1585118907018,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "OjXRu31I0-HS",
    "outputId": "50d20d02-f3c5-40cc-e05d-d3c0f121b05a"
   },
   "source": [
    "# Build phrases and bigrams to train our word2vec model on\n",
    "phrases = gensim.models.Phrases(documents, min_count=1, threshold=1)\n",
    "bigrams = gensim.models.phrases.Phraser(phrases)\n",
    "\n",
    "# Note: we're referring to the model as DS (domain-specific). This allows us\n",
    "# to make a contrast between the pretrained word embeddings and the domain-specific ones. \n",
    "modelDS = gensim.models.Word2Vec (bigrams[documents], size=50, window=10, min_count=2, workers=12)\n",
    "modelDS.train(bigrams[documents],total_examples=len(documents), epochs=10)\n",
    "modelDS.save(\"../data/word2vec/word2vec_gswa.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the hotel reviews, we load pre-trained embeddings rather than training them due to time requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDS = gensim.models.Word2Vec.load('../data/word2vec/word2vec_gswa.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgB_ATFn1eFd"
   },
   "source": [
    "## Looking at domain-specific outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183387,
     "status": "ok",
     "timestamp": 1585118907019,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Ocw0iSJv1jZd",
    "outputId": "838e326a-a8ff-4ecc-d46e-2a701fca2465"
   },
   "outputs": [],
   "source": [
    "print(list(model.wv.vocab.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183164,
     "status": "ok",
     "timestamp": 1585118907019,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "l0CQiK_31jXl",
    "outputId": "3e28d41b-d5f7-471d-deed-3dc925945b75"
   },
   "outputs": [],
   "source": [
    "print(modelDS.wv['gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 182953,
     "status": "ok",
     "timestamp": 1585118907019,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "9Fd6M6sB1jWS",
    "outputId": "9d4f742e-4eb3-46f5-96b3-1585e1d49286"
   },
   "outputs": [],
   "source": [
    "w1 = \"gold\"\n",
    "modelDS.wv.most_similar(positive = w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 182749,
     "status": "ok",
     "timestamp": 1585118907020,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "hZajnE271jUr",
    "outputId": "6158b447-428b-4215-8db1-0007a6f42e89"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"iron\"]\n",
    "modelDS.wv.most_similar(positive = w1, topn = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 182505,
     "status": "ok",
     "timestamp": 1585118907020,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "_EhVlOOI1jRC",
    "outputId": "fb6a8fe3-5a25-4b44-f360-07720a4cd428"
   },
   "outputs": [],
   "source": [
    "# get everything related to stuff on the commodity\n",
    "w1 = [\"gold\",'commodity','ore']\n",
    "w2 = ['rock']\n",
    "modelDS.wv.most_similar(positive = w1, negative = w2, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181999,
     "status": "ok",
     "timestamp": 1585118907021,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "4TfG_p_r1jOr",
    "outputId": "f644c164-f06e-4e66-b1ee-41f65a25cf4b"
   },
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "modelDS.wv.similarity(w1 = \"gold\", w2 = \"ore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181617,
     "status": "ok",
     "timestamp": 1585118907346,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "oaltQ0Eh1jMg",
    "outputId": "b3c66c4c-484d-479a-b266-84d3b95c00ee"
   },
   "outputs": [],
   "source": [
    "# similarity between two identical words\n",
    "modelDS.wv.similarity(w1 = \"gold\", w2 = \"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181462,
     "status": "ok",
     "timestamp": 1585118907347,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "4qNuYbd11jJ9",
    "outputId": "bc326fc8-a38f-496d-afda-24cd6c0ad92d"
   },
   "outputs": [],
   "source": [
    "# similarity between two unrelated words\n",
    "modelDS.wv.similarity(w1 = \"gold\", w2 = \"rock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181337,
     "status": "ok",
     "timestamp": 1585118907348,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "VbUayj321pOz",
    "outputId": "c0f8a95d-07ff-43ab-b444-cfed0901738c"
   },
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "modelDS.wv.doesnt_match([\"gold\", \"rock\", \"copper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIW9Bl-I3KEK"
   },
   "source": [
    "### Comparing our domain-specific embeddings with embeddings trained on a different dataset (news articles).\n",
    "- What are the nearest terms to 'commodity', 'ore', 'rock', etc.\n",
    "\n",
    "Unfortunately due to the size of pretrained word embedding models they have been omitted from this comparison. However, the code below shows how to load the Google news word2vec embedding model. This model is trained on 100 billion words and is 1.6GB in size (massive!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2df1AaGp8-iI"
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "# 1.6GB trained over 100B words; dimension of 300. Cannot use in this notebook due to massive size and time required to load.\n",
    "\n",
    "#import gensim.downloader as api\n",
    "#wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPmtpYf-3KBz"
   },
   "outputs": [],
   "source": [
    "# Define function to compare top-n similariries for a given word between two embedding models.\n",
    "def compare_words(word, topn, model1, model2):\n",
    "    similarWordsModel1 = model1.wv.most_similar(positive=word, topn=topn)\n",
    "    similarWordsModel2 = model2.wv.most_similar(positive=word,topn=topn)\n",
    "\n",
    "    print(f'Top {topn} words similar to {word}\\n(format: n | model 1 | model 2 )\\n')\n",
    "    for n in range(topn):\n",
    "        print(f'{n+1} |{similarWordsModel1[n][0]:<15} | {similarWordsModel2[n][0]:<15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 180612,
     "status": "ok",
     "timestamp": 1585118907349,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "gcX31xIr3J9i",
    "outputId": "75a2f13c-b992-4679-bb57-12b6f0867b84"
   },
   "outputs": [],
   "source": [
    "# Looking at the word 'commodity'\n",
    "compare_words(word = 'commodity', topn = 5, model1 = model, model2 = modelDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 180378,
     "status": "ok",
     "timestamp": 1585118907350,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "ExSMep_7RC1S",
    "outputId": "7861088b-996e-4634-e9fb-54cf29c611a8"
   },
   "outputs": [],
   "source": [
    "# Looking at the word 'ore'\n",
    "compare_words(word = 'ore', topn = 5, model1 = model, model2 = modelDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 180154,
     "status": "ok",
     "timestamp": 1585118907350,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "_3b-ykPERCyV",
    "outputId": "075f24e4-2e76-4893-81f2-1734097b95b1"
   },
   "outputs": [],
   "source": [
    "# Looking at the word 'rock'\n",
    "compare_words(word = 'rock', topn = 5, model1 = model, model2 = modelDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUFDEGVK1pMd"
   },
   "source": [
    "### Visualising the domain-specific word vectors in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhE0JjYr1pII"
   },
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,50), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.title(f'Words closest to: {word}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1472,
     "status": "ok",
     "timestamp": 1585118947324,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "qnoeIGsI1pGC",
    "outputId": "b1d2125a-20b0-4bbe-b25b-678e820ec8a0"
   },
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model = modelDS.wv, word = \"gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pzvw14B8TL7A"
   },
   "source": [
    "#### Visualising all of the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1585118951914,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "NiouXHvb1pEB",
    "outputId": "6831cc9a-22c5-474f-8c9d-127582d72f83"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(type(modelDS.wv.vocab))\n",
    "X = modelDS[modelDS.wv.vocab]\n",
    "# Shape of our model before t-SNE\n",
    "print(f'Shape of model before t-SNE: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 341654,
     "status": "ok",
     "timestamp": 1585121616330,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "tmnytvul10Xf",
    "outputId": "e6fe0879-b67b-4e98-b9dc-e5cd8a49078e"
   },
   "outputs": [],
   "source": [
    "# Fitting subset of data to t-SNE\n",
    "points = 2500\n",
    "X_limited = X[:points]\n",
    "X_tsne = tsne.fit_transform(X_limited)\n",
    "\n",
    "# Uncomment line below to fit entire model to t-SNE\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfCESffv10TD"
   },
   "source": [
    "#### Interactive Visualisation\n",
    "\n",
    "Refer to https://www.datascience.com/resources/notebooks/word-embeddings-in-python, and also for ideas of incoporating POS and bigrams into word2vec training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-YfpTUH10Qq"
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viZhA7RO10N8"
   },
   "outputs": [],
   "source": [
    "def interactive_tsne(text_labels, tsne_array):\n",
    "    '''makes an interactive scatter plot with text labels for each point'''\n",
    "\n",
    "    # Define a dataframe to be used by bokeh context\n",
    "    bokeh_df = pd.DataFrame(tsne_array, text_labels, columns=['x','y'])\n",
    "    bokeh_df['text_labels'] = bokeh_df.index\n",
    "\n",
    "    # interactive controls to include to the plot\n",
    "    TOOLS=\"hover, zoom_in, zoom_out, box_zoom, undo, redo, reset, box_select\"\n",
    "\n",
    "    p = figure(tools=TOOLS, plot_width=700, plot_height=700)\n",
    "\n",
    "    # define data source for the plot\n",
    "    source = ColumnDataSource(bokeh_df)\n",
    "\n",
    "    # scatter plot\n",
    "    p.scatter('x', 'y', source=source, fill_alpha=0.6,\n",
    "              fill_color=\"#8724B5\",\n",
    "              line_color=None)\n",
    "\n",
    "    # text labels\n",
    "    labels = LabelSet(x='x', y='y', text='text_labels', y_offset=8,\n",
    "                      text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                      source=source, text_align='center')\n",
    "\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    # show plot inline\n",
    "    output_notebook()\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch interactive visualisation of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRvKJ6gK10MD"
   },
   "outputs": [],
   "source": [
    "interactive_tsne(list(modelDS.wv.vocab.keys())[:points], X_tsne)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "12.2.2-WordEmbeddings.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
